# -*- coding: utf-8 -*-
"""CRCvsIBD.ipynb

Automatically generated by Colab.

"""

import pandas as pd


!pip install torch-geometric
!pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-1.13.0+cu116.html # Install with CUDA support

pip install adjustText

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from adjustText import adjust_text
import networkx as nx
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data
import community.community_louvain as community_louvain

import networkx as nx, itertools
from sklearn.metrics import adjusted_rand_score
import community.community_louvain as cl  # python-louvain




# Drop rows with missing values in key columns
df = data_ibd[['Gene.symbol', 'logFC', 'adj.P.Val']].dropna()

# Create a new column for -log10(adj.P.Val)
df['-log10(padj)'] = -np.log10(df['adj.P.Val'])

# Define thresholds
pvalue_threshold = 0.5  # -log10(0.01)
log2fc_threshold_low = -0.5
log2fc_threshold_high = 0.5

# Create masks for different significance levels
below_threshold = df['-log10(padj)'] < pvalue_threshold
above_threshold = df['-log10(padj)'] >= pvalue_threshold

upregulated = (df['logFC'] > log2fc_threshold_high) & above_threshold
downregulated = (df['logFC'] < log2fc_threshold_low) & above_threshold
borderline = (df['logFC'] >= log2fc_threshold_low) & (df['logFC'] <= log2fc_threshold_high) & above_threshold
# Save all upregulated and downregulated genes into separate CSV files
upregulated.to_csv("all_upregulated_genes.csv", index=False)
downregulated.to_csv("all_downregulated_genes.csv", index=False)
# Select 50 non-overlapping genes for labeling
selected_genes = df.loc[above_threshold, 'Gene.symbol'].drop_duplicates().sample(n=50, random_state=42)

# Get top upregulated and downregulated genes
top_upregulated = df.loc[upregulated].nlargest(25, 'logFC')[['Gene.symbol', 'logFC']].drop_duplicates()
top_downregulated = df.loc[downregulated].nsmallest(2500, 'logFC')[['Gene.symbol', 'logFC']].drop_duplicates()

# Ensure names do not overlap
top_upregulated = top_upregulated[~top_upregulated['Gene.symbol'].isin(top_downregulated['Gene.symbol'])]
top_downregulated = top_downregulated[~top_downregulated['Gene.symbol'].isin(top_upregulated['Gene.symbol'])]

# Print top genes
print("Top Upregulated Genes:")
print(top_upregulated)
print("\nTop Downregulated Genes:")
print(top_downregulated)


# Drop rows with missing values in key columns
df = data_ibd[['Gene.symbol', 'logFC', 'adj.P.Val']].dropna()

# Create a new column for -log10(adj.P.Val)
df['-log10(padj)'] = -np.log10(df['adj.P.Val'])

# Define thresholds
pvalue_threshold = 0.5  # -log10(0.01)
log2fc_threshold_low = -0.2
log2fc_threshold_high = 0.2

# Create masks for different significance levels
below_threshold = df['-log10(padj)'] < pvalue_threshold
above_threshold = df['-log10(padj)'] >= pvalue_threshold

upregulated = (df['logFC'] > log2fc_threshold_high) & above_threshold
downregulated = (df['logFC'] < log2fc_threshold_low) & above_threshold
borderline = (df['logFC'] >= log2fc_threshold_low) & (df['logFC'] <= log2fc_threshold_high) & above_threshold

# Select top upregulated and downregulated genes for labeling
top_upregulated = df.loc[upregulated].nlargest(25, 'logFC')[['Gene.symbol', 'logFC', '-log10(padj)']]
top_downregulated = df.loc[downregulated].nsmallest(25, 'logFC')[['Gene.symbol', 'logFC', '-log10(padj)']]

# Merge top genes into one dataframe for annotation
top_genes = pd.concat([top_upregulated, top_downregulated])

# Plot the volcano plot
plt.figure(figsize=(10, 8))

# Scatter plots for different groups
plt.scatter(df['logFC'][below_threshold], df['-log10(padj)'][below_threshold], color='grey', alpha=0.5, label='Non-significant')
plt.scatter(df['logFC'][upregulated], df['-log10(padj)'][upregulated], color='green', alpha=0.8, label='Upregulated')
plt.scatter(df['logFC'][downregulated], df['-log10(padj)'][downregulated], color='red', alpha=0.8, label='Downregulated')
plt.scatter(df['logFC'][borderline], df['-log10(padj)'][borderline], color='blue', alpha=0.8, label='Borderline significant')

# Annotate selected top genes
# Annotate selected top genes with arrows
texts = []
for _, row in top_genes.iterrows():
    texts.append(plt.text(row['logFC'], row['-log10(padj)'], row['Gene.symbol'], fontsize=8, alpha=0.7))

# Adjust text to avoid overlap and ensure arrows appear
adjust_text(texts, arrowprops=dict(arrowstyle="->", color='black', lw=0.5))

# Add threshold lines
plt.axhline(y=pvalue_threshold, color='black', linestyle='--')
plt.axvline(x=log2fc_threshold_low, color='black', linestyle='--')
plt.axvline(x=log2fc_threshold_high, color='black', linestyle='--')

plt.xlabel('log2 Fold Change', fontweight='bold')
plt.ylabel('-log10(p-adj)', fontweight='bold')
plt.title(' IBD vs Controls', fontweight='bold')

# Set plot limits
plt.xlim(left=min(df['logFC']) - 1, right=max(df['logFC']) + 1)
plt.ylim(bottom=0, top=max(df['-log10(padj)']) + 1)

# Add legend and display the plot
plt.legend()
plt.show()


# Drop rows with missing values in key columns
df = data_ibd[['Gene.symbol', 'logFC', 'adj.P.Val']].dropna()

# Create a new column for -log10(adj.P.Val)
df['-log10(padj)'] = -np.log10(df['adj.P.Val'])

# Define thresholds
pvalue_threshold = 0.5  # -log10(0.01)
log2fc_threshold_low = -0.2
log2fc_threshold_high = 0.2

# Create masks for different significance levels
below_threshold = df['-log10(padj)'] < pvalue_threshold
above_threshold = df['-log10(padj)'] >= pvalue_threshold

upregulated = (df['logFC'] > log2fc_threshold_high) & above_threshold
downregulated = (df['logFC'] < log2fc_threshold_low) & above_threshold
borderline = (df['logFC'] >= log2fc_threshold_low) & (df['logFC'] <= log2fc_threshold_high) & above_threshold

 # Extract the significant genes
upregulated_genes_ibd = df.loc[upregulated, ['Gene.symbol', 'logFC', '-log10(padj)']] # Include logFC and -log10(padj)
downregulated_genes_ibd = df.loc[downregulated, ['Gene.symbol', 'logFC', '-log10(padj)']] # Include logFC and -log10(padj)


# Create DataFrame for significant genes
top_genes = pd.concat([upregulated_genes_ibd, downregulated_genes_ibd])
# Rename the 'Gene.symbol' column to 'Gene' for consistency
top_genes.rename(columns={'Gene.symbol': 'Gene'}, inplace=True)


# Plot the volcano plot
plt.figure(figsize=(10, 8))

# Scatter plots for different groups
plt.scatter(df['logFC'][below_threshold], df['-log10(padj)'][below_threshold], color='grey', alpha=0.5, label='Non-significant')
plt.scatter(df['logFC'][upregulated], df['-log10(padj)'][upregulated], color='green', alpha=0.8, label='Upregulated')
plt.scatter(df['logFC'][downregulated], df['-log10(padj)'][downregulated], color='red', alpha=0.8, label='Downregulated')
plt.scatter(df['logFC'][borderline], df['-log10(padj)'][borderline], color='blue', alpha=0.8, label='Borderline significant')


# Add threshold lines
plt.axhline(y=pvalue_threshold, color='black', linestyle='--')
plt.axvline(x=log2fc_threshold_low, color='black', linestyle='--')
plt.axvline(x=log2fc_threshold_high, color='black', linestyle='--')

plt.xlabel('log2 Fold Change', fontweight='bold')
plt.ylabel('-log10(p-adj)', fontweight='bold')
plt.title('IBD vs Controls', fontweight='bold')

# Set plot limits
plt.xlim(left=min(df['logFC']) - 1, right=max(df['logFC']) + 1)
plt.ylim(bottom=0, top=max(df['-log10(padj)']) + 1)

# Add legend and display the plot
plt.legend()
plt.show()
 # Save the significant genes to an Excel file
top_genes.to_excel('/content/drive/My Drive/top_genes_IBD1.xlsx', index=False)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Drop rows with missing values in key columns
df_CRC = data_CRC[['Gene.symbol', 'logFC', 'adj.P.Val']].dropna()

# Create a new column for -log10(adj.P.Val)
df_CRC['-log10(padj)'] = -np.log10(df_CRC['adj.P.Val'])

# Define thresholds
pvalue_threshold = 0.5  # -log10(0.01)
log2fc_threshold_low = -0.2
log2fc_threshold_high = 0.2

# Create masks for different significance levels
below_threshold = df_CRC['-log10(padj)'] < pvalue_threshold
above_threshold = df_CRC['-log10(padj)'] >= pvalue_threshold

upregulated = (df_CRC['logFC'] > log2fc_threshold_high) & above_threshold
downregulated = (df_CRC['logFC'] < log2fc_threshold_low) & above_threshold
borderline = (df_CRC['logFC'] >= log2fc_threshold_low) & (df_CRC['logFC'] <= log2fc_threshold_high) & above_threshold

# Select 50 non-overlapping genes for labeling
selected_genes = df_CRC.loc[above_threshold, 'Gene.symbol'].drop_duplicates().sample(n=50, random_state=42)

# Get top upregulated and downregulated genes
top_upregulated = df_CRC.loc[upregulated].nlargest(25, 'logFC')[['Gene.symbol', 'logFC']].drop_duplicates()
top_downregulated = df_CRC.loc[downregulated].nsmallest(25, 'logFC')[['Gene.symbol', 'logFC']].drop_duplicates()

# Ensure names do not overlap
top_upregulated = top_upregulated[~top_upregulated['Gene.symbol'].isin(top_downregulated['Gene.symbol'])]
top_downregulated = top_downregulated[~top_downregulated['Gene.symbol'].isin(top_upregulated['Gene.symbol'])]

# Print top genes
print("Top Upregulated Genes:")
print(top_upregulated)
print("\nTop Downregulated Genes:")
print(top_downregulated)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from adjustText import adjust_text  # Import for automatic text adjustment


# Drop rows with missing values in key columns
df = data_CRC[['Gene.symbol', 'logFC', 'adj.P.Val']].dropna()

# Create a new column for -log10(adj.P.Val)
df['-log10(padj)'] = -np.log10(df['adj.P.Val'])

# Define thresholds
pvalue_threshold = 0.5  # -log10(0.01)
log2fc_threshold_low = -0.2
log2fc_threshold_high = 0.2

# Create masks for different significance levels
below_threshold = df['-log10(padj)'] < pvalue_threshold
above_threshold = df['-log10(padj)'] >= pvalue_threshold

upregulated = (df['logFC'] > log2fc_threshold_high) & above_threshold
downregulated = (df['logFC'] < log2fc_threshold_low) & above_threshold
borderline = (df['logFC'] >= log2fc_threshold_low) & (df['logFC'] <= log2fc_threshold_high) & above_threshold

 # Extract the significant genes
upregulated_genes_crc = df.loc[upregulated, ['Gene.symbol', 'logFC', '-log10(padj)']] # Include logFC and -log10(padj)
downregulated_genes_crc = df.loc[downregulated, ['Gene.symbol', 'logFC', '-log10(padj)']] # Include logFC and -log10(padj)


# Create DataFrame for significant genes
top_genes_crc = pd.concat([upregulated_genes_crc, downregulated_genes_crc])
# Rename the 'Gene.symbol' column to 'Gene' for consistency
top_genes_crc.rename(columns={'Gene.symbol': 'Gene'}, inplace=True)


# Plot the volcano plot
plt.figure(figsize=(10, 8))

# Scatter plots for different groups
plt.scatter(df['logFC'][below_threshold], df['-log10(padj)'][below_threshold], color='grey', alpha=0.5, label='Non-significant')
plt.scatter(df['logFC'][upregulated], df['-log10(padj)'][upregulated], color='green', alpha=0.8, label='Upregulated')
plt.scatter(df['logFC'][downregulated], df['-log10(padj)'][downregulated], color='red', alpha=0.8, label='Downregulated')
plt.scatter(df['logFC'][borderline], df['-log10(padj)'][borderline], color='blue', alpha=0.8, label='Borderline significant')


# Add threshold lines
plt.axhline(y=pvalue_threshold, color='black', linestyle='--')
plt.axvline(x=log2fc_threshold_low, color='black', linestyle='--')
plt.axvline(x=log2fc_threshold_high, color='black', linestyle='--')

plt.xlabel('log2 Fold Change', fontweight='bold')
plt.ylabel('-log10(p-adj)', fontweight='bold')
plt.title('CRC vs Controls', fontweight='bold')

# Set plot limits
plt.xlim(left=min(df['logFC']) - 1, right=max(df['logFC']) + 1)
plt.ylim(bottom=0, top=max(df['-log10(padj)']) + 1)

# Add legend and display the plot
plt.legend()
plt.show()
 # Save the significant genes to an Excel file
top_genes_crc.to_excel('/content/drive/My Drive/top_genes_crc1.xlsx', index=False)

# Define thresholds
log2fc_threshold_high = 0.2  # Threshold for upregulated genes
log2fc_threshold_low = -0.2  # Threshold for downregulated genes

# Extract upregulated and downregulated genes for IBD
upregulated_ibd = top_genes[top_genes['logFC'] > log2fc_threshold_high]['Gene'].tolist()
downregulated_ibd = top_genes[top_genes['logFC'] < log2fc_threshold_low]['Gene'].tolist()

# Extract upregulated and downregulated genes for CRC
upregulated_crc = top_genes_crc[top_genes_crc['logFC'] > log2fc_threshold_high]['Gene'].tolist()
downregulated_crc = top_genes_crc[top_genes_crc['logFC'] < log2fc_threshold_low]['Gene'].tolist()

# Find common upregulated genes
common_upregulated = list(set(upregulated_ibd).intersection(set(upregulated_crc)))

# Find common downregulated genes
common_downregulated = list(set(downregulated_ibd).intersection(set(downregulated_crc)))

# Combine common upregulated and downregulated genes
common_genes = common_upregulated + common_downregulated

# Print results
print("Common Upregulated Genes:", common_upregulated)
print("Common Downregulated Genes:", common_downregulated)
print("All Common Genes:", common_genes)

# Counts for IBD
count_ibd_up = len(upregulated_genes_ibd)
count_ibd_down = len(downregulated_genes_ibd)

# Counts for CRC
count_crc_up = len(upregulated_genes_crc)
count_crc_down = len(downregulated_genes_crc)

# Common genes overall (regardless of regulation)
ibd_genes = set(top_genes['Gene'])
crc_genes = set(top_genes_crc['Gene'])
common_all = ibd_genes.intersection(crc_genes)
count_common_all = len(common_all)

# Common upregulated genes
common_upregulated = set(upregulated_genes_ibd['Gene.symbol']).intersection(set(upregulated_genes_crc['Gene.symbol']))
count_common_up = len(common_upregulated)

# Common downregulated genes
common_downregulated = set(downregulated_genes_ibd['Gene.symbol']).intersection(set(downregulated_genes_crc['Gene.symbol']))
count_common_down = len(common_downregulated)

print(f"IBD Upregulated Genes: {count_ibd_up}")
print(f"IBD Downregulated Genes: {count_ibd_down}")
print(f"CRC Upregulated Genes: {count_crc_up}")
print(f"CRC Downregulated Genes: {count_crc_down}")
print(f"Common Genes (all): {count_common_all}")
print(f"Common Upregulated Genes: {count_common_up}")
print(f"Common Downregulated Genes: {count_common_down}")

!pip install upsetplot

import pandas as pd
from upsetplot import from_contents, plot
import matplotlib.pyplot as plt


# For demonstration, create example sets with size info
ibd_up_genes = set(f'IBD_UP_{i}' for i in range(3539))
ibd_down_genes = set(f'IBD_DOWN_{i}' for i in range(3189))
crc_up_genes = set(f'CRC_UP_{i}' for i in range(11001))
crc_down_genes = set(f'CRC_DOWN_{i}' for i in range(13811))


# Construct dictionary of gene sets
contents = {
    "IBD Upregulated": ibd_up_genes,
    "IBD Downregulated": ibd_down_genes,
    "CRC Upregulated": crc_up_genes,
    "CRC Downregulated": crc_down_genes
}

# Build the upset plot data
upset_data = from_contents(contents)

# Plot UpSet plot
plt.figure(figsize=(10,6))
plot(upset_data, show_counts='%d', sort_by='cardinality')
plt.title("UpSet Plot of IBD and CRC Up/Downregulated Genes")
plt.show()

# Convert to sets of significant genes (all up + down)
ibd_genes_set = set(top_genes['Gene'])  # IBD significant genes
crc_genes_set = set(top_genes_crc['Gene'])  # CRC significant genes

common_genes_set = ibd_genes_set.intersection(crc_genes_set)
print(f"Number of common significant genes: {len(common_genes_set)}")

common_genes_list = list(common_genes_set)

# Filter IBD data
df_ibd_common = df[df['Gene.symbol'].isin(common_genes_list)].copy()

# Filter CRC data
df_crc_common = df_CRC[df_CRC['Gene.symbol'].isin(common_genes_list)].copy()

import pandas as pd

# Create a DataFrame for common genes
common_genes_df = pd.DataFrame({
    'Gene.symbol': common_genes,
    'Category': ['Upregulated'] * len(common_upregulated) + ['Downregulated'] * len(common_downregulated)
})
# Save to a CSV file
common_genes_df.to_csv('/content/common_genes_ibd_crc.csv', index=False)

import requests

def get_string_interactions(gene_list, species=9606, confidence=0.5, batch_size=200):
    
    url = "https://string-db.org/api/json/network"
    all_interactions = []

    for i in range(0, len(gene_list), batch_size):
        batch = gene_list[i : i + batch_size]  # Process genes in batches
        params = {
            "identifiers": "%0d".join(batch),
            "species": species,
            "required_score": int(confidence * 1000),
            "caller_identity": "your_email@example.com",  # Replace with your email
        }
        response = requests.get(url, params=params)
        if response.status_code == 200:
            all_interactions.extend(response.json())  # Add interactions to the list
        else:
            raise Exception(f"Error fetching data: {response.status_code}")

    return all_interactions  # Return the combined interactions

# Fetch interactions for common genes in batches
interactions = get_string_interactions(common_genes, confidence=0.5)


def build_ppi_network(interactions):
    """
    Build a PPI network from STRING interactions.

    Parameters:
        interactions (list): List of interactions from STRING.

    Returns:
        nx.Graph: PPI network.
    """
    G = nx.Graph()
    for interaction in interactions:
        # Add edges between interacting proteins
        G.add_edge(interaction['preferredName_A'], interaction['preferredName_B'], weight=interaction['score'])
    return G

# Build the PPI network
ppi_network = build_ppi_network(interactions)

# Print basic network information
print(f"Number of nodes: {ppi_network.number_of_nodes()}")
print(f"Number of edges: {ppi_network.number_of_edges()}")

def visualize_ppi_network(G):
    """
    Visualize the PPI network.

    Parameters:
        G (nx.Graph): PPI network.
    """
    plt.figure(figsize=(10, 8))

    # Use a spring layout for better visualization
    pos = nx.spring_layout(G)

    # Draw nodes and edges
    nx.draw_networkx_nodes(G, pos, node_size=50, node_color='skyblue', alpha=0.8)
    nx.draw_networkx_edges(G, pos, edge_color='black', alpha=0.2)


    # Add title and display
    plt.title("PPI Network of Common Genes", fontsize=14)
    plt.axis('off')  # Hide axes
    plt.show()

# Visualize the network
visualize_ppi_network(ppi_network)

# Function to compute centrality measures
def compute_centrality(G):
    degree_centrality = nx.degree_centrality(G)
    betweenness_centrality = nx.betweenness_centrality(G, weight="weight")
    closeness_centrality = nx.closeness_centrality(G)
    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000, weight="weight")
    clustering_coefficient = nx.clustering(G, weight="weight")
    pagerank_centrality = nx.pagerank(G, weight="weight")
    harmonic_centrality = nx.harmonic_centrality(G)
    subgraph_centrality = nx.subgraph_centrality(G)

    # Compute bridging centrality (edge betweenness-based approach)
    edge_betweenness = nx.edge_betweenness_centrality(G, weight="weight")
    bridging_centrality = {}
    for node in G.nodes():
        # Modified to handle edge representation correctly
        bridging_centrality[node] = sum(edge_betweenness.get(tuple(sorted((node, neighbor))), 0)  # Handle missing edges
                                          for neighbor in G.neighbors(node))

    # Store results in a DataFrame
    centrality_df = pd.DataFrame({
        "Node": list(G.nodes()),
        "Degree": [degree_centrality[node] for node in G.nodes()],
        "Betweenness": [betweenness_centrality[node] for node in G.nodes()],
        "Closeness": [closeness_centrality[node] for node in G.nodes()],
        "Eigenvector": [eigenvector_centrality[node] for node in G.nodes()],
        "Clustering Coefficient": [clustering_coefficient[node] for node in G.nodes()],
        "PageRank": [pagerank_centrality[node] for node in G.nodes()],
        "Harmonic": [harmonic_centrality[node] for node in G.nodes()],
        "Subgraph": [subgraph_centrality[node] for node in G.nodes()],
        "Bridging": [bridging_centrality[node] for node in G.nodes()],
    })

    # Save to CSV
    centrality_df.to_csv("ppi_centrality.csv", index=False)

    return centrality_df

G = build_ppi_network(interactions)

# Compute centrality
centrality_df = compute_centrality(G)
print(centrality_df.head())

# Function to compute centrality measures

def compute_centrality(G):
    degree_centrality = nx.degree_centrality(G)
    betweenness_centrality = nx.betweenness_centrality(G, weight="weight")
    closeness_centrality = nx.closeness_centrality(G)
    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000, weight="weight")
    clustering_coefficient = nx.clustering(G, weight="weight")
    pagerank = nx.pagerank(G, weight="weight")
    harmonic_centrality = nx.harmonic_centrality(G)
    subgraph_centrality = nx.subgraph_centrality(G)
    bridging_centrality = {node: betweenness_centrality[node] / (degree_centrality[node] + 1e-6) for node in G.nodes()}

    # Store results in a DataFrame
    centrality_df = pd.DataFrame({
        "Node": list(G.nodes()),
        "Degree": [degree_centrality[node] for node in G.nodes()],
        "Betweenness": [betweenness_centrality[node] for node in G.nodes()],
        "Closeness": [closeness_centrality[node] for node in G.nodes()],
        "Eigenvector": [eigenvector_centrality[node] for node in G.nodes()],
        "Clustering Coefficient": [clustering_coefficient[node] for node in G.nodes()],
        "PageRank": [pagerank[node] for node in G.nodes()],
        "Harmonic": [harmonic_centrality[node] for node in G.nodes()],
        "Subgraph": [subgraph_centrality[node] for node in G.nodes()],
        "Bridging": [bridging_centrality[node] for node in G.nodes()],
    })

    # Save to CSV
    centrality_df.to_csv("ppi_centrality.csv", index=False)
    return centrality_df

# Function to compute average centrality per node
def compute_average_centrality(G):
    centrality_df = compute_centrality(G)
    avg_centrality = centrality_df.set_index("Node").mean(axis=1).to_dict()
    return avg_centrality

# Function to compute adjacency matrix and multiply with centrality matrix
def multiply_centrality_with_adjacency(G):
    avg_centrality = compute_average_centrality(G)
    A = nx.to_numpy_array(G, nodelist=sorted(G.nodes()), weight="weight")
    C = np.diag([avg_centrality[node] for node in sorted(G.nodes())])
    result = np.dot(C, A)
    return result, avg_centrality

G = build_ppi_network(interactions)

# Compute modified adjacency matrix
modified_adj_matrix, avg_centrality = multiply_centrality_with_adjacency(G)

# Convert to DataFrame and save
nodes = sorted(G.nodes())
df_modified_adj = pd.DataFrame(modified_adj_matrix, index=nodes, columns=nodes)
df_modified_adj.to_csv("modified_adjacency_matrix.csv")
print("Modified adjacency matrix saved as 'modified_adjacency_matrix.csv'")


# Function to normalize a matrix row-wise
def normalize_matrix(matrix):
    row_sums = matrix.sum(axis=1, keepdims=True)  # Sum of each row
    normalized_matrix = matrix / row_sums  # Element-wise division
    normalized_matrix[np.isnan(normalized_matrix)] = 0  # Handle division by zero
    return normalized_matrix

# Compute modified adjacency matrix
modified_adj_matrix, avg_centrality = multiply_centrality_with_adjacency(G)

# Normalize the adjacency matrix
normalized_adj_matrix = normalize_matrix(modified_adj_matrix)
normalized_adj_matrix
# Convert to DataFrame and save
nodes = sorted(G.nodes())
df_normalized_adj = pd.DataFrame(normalized_adj_matrix, index=nodes, columns=nodes)
df_normalized_adj.to_csv("normalized_adjacency_matrix.csv")
normalized_adj_matrix
print(df_modified_adj.head())


# Load the normalized adjacency matrix and centrality features
adj_matrix = torch.tensor(normalized_adj_matrix, dtype=torch.float32)
features = torch.tensor(list(avg_centrality.values()), dtype=torch.float32).view(-1, 1)

# Define labels (influence scores) - Replace with actual labels if available
labels = features.clone()  # Placeholder, replace with actual labels

# Get the number of nodes directly from the adjacency matrix
num_nodes = adj_matrix.shape[0]

# Convert adjacency matrix to edge index
edge_index = torch.tensor(np.array(np.nonzero(adj_matrix)), dtype=torch.long)

# Ensure `edge_index` has shape (2, num_edges)
if edge_index.shape[0] != 2:
    edge_index = edge_index.T  # Transpose to fix shape

# Generate self-loops (each node connects to itself)
self_loops = torch.arange(edge_index.max() + 1).repeat(2, 1)

# Concatenate the original edge index and self-loops correctly
edge_index = torch.cat([edge_index, self_loops], dim=1)  # Use dim=1 (columns) instead of dim=0

# Define PyTorch Geometric Graph Data
graph_data = Data(x=features, edge_index=edge_index, y=labels)

# Define GNN Model (GCN)
class GNN(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GNN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Initialize model, optimizer, and loss function
model = GNN(input_dim=1, hidden_dim=16, output_dim=1)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
loss_fn = torch.nn.MSELoss()

# Training loop
def train(model, data, epochs=200):
    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        output = model(data.x, data.edge_index)
        loss = loss_fn(output, data.y)
        loss.backward()
        optimizer.step()
        if epoch % 20 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item()}")

# Train the model
train(model, graph_data)

# Predict node influence
model.eval()
predictions = model(graph_data.x, graph_data.edge_index).detach().numpy()
print("Predicted Node Influence:", predictions[:10])

import pandas as pd

# Get sorted node list (same order as adjacency matrix)
nodes = sorted(G.nodes())

# Convert predictions to a NumPy array
predicted_influence = predictions.flatten()

# Create a DataFrame with Gene Names and Influence Scores
df_influence = pd.DataFrame({"Gene": nodes, "Influence_Score": predicted_influence})

# Sort by Influence Score in descending order
df_influence = df_influence.sort_values(by="Influence_Score", ascending=False)

# Save to CSV
df_influence.to_csv("sorted_predicted_node_influence.csv", index=False)

# Print top 10 genes with highest influence
print(df_influence.head(100))
# Save only top 100 influential genes to a separate CSV
df_influence.head(100).to_csv("top_100_influential_genes.csv", index=False)

pip install louvain

!pip uninstall -y community
!pip install -U python-louvain


partition = community_louvain.best_partition(G, resolution=1.0, random_state=42)

# Convert partition dict to DataFrame for easier analysis
df_communities = pd.DataFrame(list(partition.items()), columns=["Gene", "Community"])

# Save communities to CSV
df_communities.to_csv("louvain_communities.csv", index=False)

# Optionally, print number of communities and some stats
num_communities = len(set(partition.values()))
print(f"Number of communities detected: {num_communities}")

# Example: show top 5 nodes from each community
for community_id in set(partition.values()):
    print(f"Community {community_id}:")
    community_nodes = [node for node, comm in partition.items() if comm == community_id]
    print(community_nodes[:5])
    print()

from collections import Counter

# Count the number of nodes in each community
community_sizes = Counter(partition.values())

# Get top 10 largest communities by size
top_10_communities = community_sizes.most_common(10)

print("Top 10 communities by size:")
for community_id, size in top_10_communities:
    print(f"Community {community_id}: {size} genes")

# If you want to list top 5 genes in each of these top 10 communities:
for community_id, size in top_10_communities:
    community_nodes = [node for node, comm in partition.items() if comm == community_id]
    print(f"\nCommunity {community_id} (size: {size}): Top 5 genes")
    print(community_nodes[:50])

import pandas as pd
from collections import Counter

# Count the size of each community
community_sizes = Counter(partition.values())

# Get top 10 largest communities by size
top_10_communities = [comm_id for comm_id, size in community_sizes.most_common(10)]

# Prepare data for saving: list of (Community, Gene)
rows = []
for comm_id in top_10_communities:
    community_nodes = [node for node, comm in partition.items() if comm == comm_id]
    for gene in community_nodes:
        rows.append({"Community": comm_id, "Gene": gene})

# Create DataFrame
df_top_10_communities = pd.DataFrame(rows)

# Save to CSV
df_top_10_communities.to_csv("top_10_louvain_communities_genes.csv", index=False)

print("Saved top 10 communities with genes to 'top_10_louvain_communities_genes.csv'")


# ----------------------------
# Hub selectors (baselines)
# ----------------------------
def topk_degree(G, k=50):
    c = nx.degree_centrality(G)
    return [n for n,_ in sorted(c.items(), key=lambda x: x[1], reverse=True)[:k]]

def topk_betweenness(G, k=50):
    c = nx.betweenness_centrality(G, weight="weight")
    return [n for n,_ in sorted(c.items(), key=lambda x: x[1], reverse=True)[:k]]

def topk_eigenvector(G, k=50):
    try:
        c = nx.eigenvector_centrality(G, max_iter=2000, weight="weight")
    except nx.PowerIterationFailedConvergence:
        # fall back to unweighted if weighted fails
        c = nx.eigenvector_centrality(G, max_iter=2000)
    return [n for n,_ in sorted(c.items(), key=lambda x: x[1], reverse=True)[:k]]

def topk_random(G, k=50, seed=42):
    rng = np.random.RandomState(seed)
    nodes = list(G.nodes())
    if len(nodes) <= k: return nodes
    return list(rng.choice(nodes, size=k, replace=False))

# Optional: your equal-weight composite (degree, betweenness, closeness, eigenvector)
def topk_composite_equal(G, k=50):
    deg = pd.Series(nx.degree_centrality(G))
    bet = pd.Series(nx.betweenness_centrality(G, weight="weight"))
    clo = pd.Series(nx.closeness_centrality(G))
    try:
        eig = pd.Series(nx.eigenvector_centrality(G, max_iter=2000, weight="weight"))
    except nx.PowerIterationFailedConvergence:
        eig = pd.Series(nx.eigenvector_centrality(G, max_iter=2000))
    # min–max normalize each
    comp = (_safe_norm(deg) + _safe_norm(bet) + _safe_norm(clo) + _safe_norm(eig)) / 4.0
    return list(comp.sort_values(ascending=False).head(k).index)

# ----------------------------
# One runner to compare methods
# ----------------------------
def compare_baselines(G, k=50, our_hubs=None, part=None, seed=42):
    if part is None:
        part = louvain_partition(G, resolution=1.0, seed=seed)

    methods = {
        "Degree-Topk":      lambda GG, kk: topk_degree(GG, kk),
        "Betweenness-Topk": lambda GG, kk: topk_betweenness(GG, kk),
        "Eigenvector-Topk": lambda GG, kk: topk_eigenvector(GG, kk),
        "Composite-Equal":  lambda GG, kk: topk_composite_equal(GG, kk),
        "Random-Topk":      lambda GG, kk: topk_random(GG, kk, seed=seed),
    }

    rows = []
    for name, hubfunc in methods.items():
        hubs = hubfunc(G, k)
        # basic centralities for reporting
        deg_vals = dict(G.degree(weight=None))
        try:
            eig_vals = nx.eigenvector_centrality(G, max_iter=2000, weight="weight")
        except nx.PowerIterationFailedConvergence:
            eig_vals = nx.eigenvector_centrality(G, max_iter=2000)
        mean_deg = float(np.mean([deg_vals.get(h, 0) for h in hubs])) if hubs else np.nan
        mean_eig = float(np.mean([eig_vals.get(h, 0.0) for h in hubs])) if hubs else np.nan

        coh = community_cohesion(hubs, part)
        Q   = induced_modularity(G, hubs, seed=seed)
        stab_mean, stab_sd = bootstrap_stability(G, hubfunc, k=k, iters=50, edge_keep_p=0.8, seed=seed)

        # overlap metrics vs. our set (if provided)
        if our_hubs is not None and len(our_hubs) > 0:
            ov_count = len(set(hubs) & set(our_hubs))
            ov_jac   = jaccard(hubs, our_hubs)
        else:
            ov_count = np.nan
            ov_jac   = np.nan

        rows.append({
            "Method": name,
            "k": k,
            "Overlap_with_Ours": ov_count,
            "Jaccard_with_Ours": round(ov_jac, 3) if not np.isnan(ov_jac) else np.nan,
            "Cohesion_%": round(coh, 1) if not np.isnan(coh) else np.nan,
            "Modularity_Q": round(Q, 3) if not np.isnan(Q) else np.nan,
            "Mean_Degree": round(mean_deg, 2) if not np.isnan(mean_deg) else np.nan,
            "Mean_Eigenvector": round(mean_eig, 4) if not np.isnan(mean_eig) else np.nan,
            "Stability_Jaccard_mean": round(stab_mean, 3) if not np.isnan(stab_mean) else np.nan,
            "Stability_Jaccard_sd": round(stab_sd, 3) if not np.isnan(stab_sd) else np.nan,
        })
    return pd.DataFrame(rows).sort_values(["Method"]).reset_index(drop=True)



# =========================
def compare_baselines_with_wgcna_cytohubba(G_ppi: nx.Graph,
                                           expr: pd.DataFrame,
                                           y=None,
                                           k=50,
                                           our_hubs=None,
                                           part=None,
                                           seed=42):
    """
    Extends your previous compare_baselines(...) by adding:
      - WGCNA-like (kME in module most associated with y)
      - cytoHubba-like (MCC)
    Uses all the same metrics you already compute.
    """
    # Reuse your existing helpers if already defined in your notebook:
    def jaccard(a, b):
        A, B = set(a), set(b)
        if len(A)==0 and len(B)==0: return 1.0
        if len(A|B)==0: return 0.0
        return len(A & B)/len(A | B)

    def louvain_partition(G, resolution=1.0, seed=42):
        import random
        random.seed(seed); np.random.seed(seed)
        return community_louvain.best_partition(G, resolution=resolution, weight="weight")

    def community_cohesion(hubs, partition):
        hubs = [h for h in hubs if h in partition]
        if len(hubs) == 0:
            return np.nan
        comms = [partition[h] for h in hubs]
        most = Counter(comms).most_common(1)[0][1]
        return 100.0 * most / len(hubs)

    def induced_modularity(G, hubs, seed=42):
        H = G.subgraph([h for h in hubs if h in G]).copy()
        if H.number_of_nodes() < 3 or H.number_of_edges() == 0:
            return np.nan
        part_H = louvain_partition(H, resolution=1.0, seed=seed)
        return community_louvain.modularity(part_H, H, weight="weight")

    def bootstrap_stability(G, hub_func, k=50, iters=50, edge_keep_p=0.8, seed=42):
        rng = np.random.RandomState(seed)
        base_hubs = hub_func(G, k)
        if len(base_hubs) == 0:
            return np.nan, np.nan
        edges = list(G.edges(data=True))
        jaccs = []
        for t in range(iters):
            kept = [e for e in edges if rng.rand() < edge_keep_p]
            H = nx.Graph(); H.add_nodes_from(G.nodes())
            H.add_edges_from((u,v,{"weight":d.get("weight",1.0)}) for u,v,d in kept)
            try:
                hubs_b = hub_func(H, k)
            except Exception:
                hubs_b = []
            jaccs.append(jaccard(base_hubs, hubs_b))
        jaccs = np.array(jaccs, float)
        return float(jaccs.mean()), float(jaccs.std(ddof=1)) if len(jaccs)>1 else (float(jaccs.mean()), np.nan)

    # If no partition provided, build one on the PPI
    if part is None:
        part = louvain_partition(G_ppi, resolution=1.0, seed=seed)

    # Existing methods you already had
    def topk_degree(G, k=50):
        c = nx.degree_centrality(G)
        return [n for n,_ in sorted(c.items(), key=lambda x: x[1], reverse=True)[:k]]

    def topk_betweenness(G, k=50):
        c = nx.betweenness_centrality(G, weight="weight")
        return [n for n,_ in sorted(c.items(), key=lambda x: x[1], reverse=True)[:k]]

    def topk_eigenvector(G, k=50):
        try:
            c = nx.eigenvector_centrality(G, max_iter=2000, weight="weight")
        except nx.PowerIterationFailedConvergence:
            c = nx.eigenvector_centrality(G, max_iter=2000)
        return [n for n,_ in sorted(c.items(), key=lambda x: x[1], reverse=True)[:k]]

    def _safe_norm(series):
        s = pd.Series(series, dtype=float)
        lo, hi = s.min(), s.max()
        if hi - lo == 0: return pd.Series(np.zeros_like(s), index=s.index)
        return (s - lo) / (hi - lo)

    def topk_composite_equal(G, k=50):
        deg = pd.Series(nx.degree_centrality(G))
        bet = pd.Series(nx.betweenness_centrality(G, weight="weight"))
        clo = pd.Series(nx.closeness_centrality(G))
        try:
            eig = pd.Series(nx.eigenvector_centrality(G, max_iter=2000, weight="weight"))
        except nx.PowerIterationFailedConvergence:
            eig = pd.Series(nx.eigenvector_centrality(G, max_iter=2000))
        comp = (_safe_norm(deg) + _safe_norm(bet) + _safe_norm(clo) + _safe_norm(eig)) / 4.0
        return list(comp.sort_values(ascending=False).head(k).index)

    # New methods
    def hubs_wgcna_like(G_unused, k=50):
        # Align expr genes to the PPI node universe for fairness
        genes_universe = expr.columns.intersection(pd.Index(G_ppi.nodes()))
        if len(genes_universe) == 0:
            return []
        hubs = wgcna_like_hubs(expr.loc[:, genes_universe], y=y, k=k, beta=6, q=0.95, min_module_size=20, seed=seed)
        # Keep hubs that are in the PPI (most should be)
        return [h for h in hubs if h in G_ppi]

    def hubs_cytohubba_mcc(G, k=50):
        return cytohubba_mcc_hubs(G, k=k, max_time_s=60, max_cliques=200000, max_fact=12)

    methods = {
        "Degree-Topk":       lambda GG, kk: topk_degree(GG, kk),
        "Betweenness-Topk":  lambda GG, kk: topk_betweenness(GG, kk),
        "Eigenvector-Topk":  lambda GG, kk: topk_eigenvector(GG, kk),
        "Composite-Equal":   lambda GG, kk: topk_composite_equal(GG, kk),
        "WGCNA-like (kME)":  lambda GG, kk: hubs_wgcna_like(GG, kk),
        "cytoHubba (MCC)":   lambda GG, kk: hubs_cytohubba_mcc(GG, kk),
    }


  
    # Helper to highlight "ours" (Composite-Equal) if present
    highlight = "Composite-Equal"
    colors = ["tab:orange" if m == highlight else "tab:blue" for m in df["Method"]]

    # --- Stability: mean ± sd ---
    if {"Stability_Jaccard_mean","Stability_Jaccard_sd"}.issubset(df.columns):
        plt.figure(figsize=(8,4))
        mean = df["Stability_Jaccard_mean"].values
        sd   = df["Stability_Jaccard_sd"].values
        plt.bar(x, mean, yerr=sd, capsize=4, color=colors, alpha=0.9)
        plt.xticks(x, df["Method"], rotation=20, ha="right")
        plt.ylabel("Stability (Jaccard mean ± SD)")
        plt.title(f"Bootstrap stability of hub selection (k = {k_val})")
        plt.tight_layout()
        plt.show()

    # --- Modularity Q of induced subgraph ---
    if "Modularity_Q" in df.columns:
        plt.figure(figsize=(8,4))
        plt.bar(x, df["Modularity_Q"].values, color=colors, alpha=0.9)
        plt.xticks(x, df["Method"], rotation=20, ha="right")
        plt.ylabel("Induced subgraph modularity (Q)")
        plt.title(f"Community structure of selected hubs (k = {k_val})")
        plt.tight_layout()
        plt.show()

    # --- Cohesion (%) in modal community ---
    if "Cohesion_%" in df.columns:
        plt.figure(figsize=(8,4))
        plt.bar(x, df["Cohesion_%"].values, color=colors, alpha=0.9)
        plt.xticks(x, df["Method"], rotation=20, ha="right")
        plt.ylabel("Cohesion (% hubs in modal community)")
        plt.title("Cohesion of hubs within Louvain partition")
        plt.tight_layout()
        plt.show()

# ---- usage (with your DataFrame 'results_df') ----
save_and_plot_baselines(results_df, outfile="baseline_summary.csv")



def louvain_sweep_metrics(G, gammas=(0.5,0.8,1.0,1.2,1.5,2.0), seeds=range(10), weight='weight'):
    rows = []
    for gamma in gammas:
        parts = []
        for s in seeds:
            # python-louvain uses global RNG; set seeds for reproducibility
            np.random.seed(s)
            p = cl.best_partition(G, resolution=gamma, weight=weight, random_state=s)
            parts.append(p)
            Q = cl.modularity(p, G, weight=weight)
            C = len(set(p.values()))
            rows.append(dict(gamma=gamma, seed=int(s), Q=float(Q), C=int(C)))
        # stability: ARI across all seed pairs at fixed gamma
        aris = []
        for (p1, p2) in itertools.combinations(parts, 2):
            # align labels to the same node order
            nodes = list(G.nodes())
            a = [p1[n] for n in nodes]
            b = [p2[n] for n in nodes]
            aris.append(adjusted_rand_score(a, b))
        rows.append(dict(gamma=gamma, seed=np.nan,
                         Q=np.nan, C=np.nan,
                         ARI_median=np.median(aris),
                         ARI_iqr_hi=np.percentile(aris, 75),
                         ARI_iqr_lo=np.percentile(aris, 25)))
    df = pd.DataFrame(rows)
    # aggregate per gamma (exclude the ARI summary rows with NaN seed for Q/C means)
    agg = (df.dropna(subset=['seed'])
             .groupby('gamma', as_index=False)
             .agg(Q_mean=('Q','mean'), Q_sd=('Q','std'),
                  C_mean=('C','mean'), C_sd=('C','std')))
    # merge ARI median/IQR
    ari = df[df['seed'].isna()][['gamma','ARI_median','ARI_iqr_lo','ARI_iqr_hi']]
    out = agg.merge(ari, on='gamma', how='left')
    return out.sort_values('gamma'), df

summary_ppi, perrun_ppi = louvain_sweep_metrics(ppi_network, gammas=(0.5,0.8,1.0,1.2,1.5,2.0))

print(summary_ppi)
summary_ppi.to_csv("louvain_resolution_sweep_ppi.csv", index=False)

